# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OF7dNPS3SsweZ7TwER605Hw5OB5ccm_p
"""

import numpy as np
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import time

class Perceptron():
    def __init__(self, eta=0.1, n_iter=10):
        self.eta = eta
        self.n_iter = n_iter

    def netInput(self, x):
        return np.dot(x,  self.w_[1:]) + self.w_[0]

    def predict(self, x):
        return np.where(self.netInput(x) >= 0.0, 1, -1)

    def fit(self, x, y):
        self.w_ = np.zeros(1 + x.shape[1])
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(x, y):
                update = self.eta * (target - self.predict(xi))
                self.w_[1:] += update * xi
                self.w_[0] += update
                errors += int(update != 0.0)
            self.errors_.append(errors)
        return self

class AdalineGD(Perceptron):
    def __init__(self, eta=0.1, n_iter=10, type_grad_descent='Batch'):
        self.type_grad_descent = type_grad_descent
        self.eta = eta
        self.n_iter = n_iter

    def netInput(self, x):
        return np.dot(x, self.w_[1:]) + self.w_[0]

    def activation(self, x):
        return self.netInput(x)

    def predict(self, x):
        return np.where(self.netInput(x)>=0.0,1,-1)

    def updateWeights(self, xi, target):
        output = self.netInput(xi)
        error = target - output
        self.w_[1:] += self.eta * xi.dot(error)
        self.w_[0] += self.eta * error
        cost = (error**2).sum() / 2.0
        return cost

    def fit(self, x, y):
        self.w_ = np.zeros(1 + x.shape[1])
        self.errors_ = []

        if self.type_grad_descent == 'Stochastic':
            for _ in range(self.n_iter):
                errors = []
                for xi, target in zip(x, y):
                    errors.append(self.updateWeights(xi, target))
                avg_errors = sum(errors)/len(y)
                self.errors_.append(avg_errors)

        elif self.type_grad_descent == 'Batch':
            for _ in range(self.n_iter):
                output = self.netInput(x)
                errors = y - output
                self.w_[1:] += self.eta * x.T.dot(errors)
                self.w_[0] += self.eta * errors.sum()
                cost = (errors**2).sum() / 2.0
                self.errors_.append(cost)

        else:
            print(f'''Такого типа просчета градиентного спуска как
                    {self.type_grad_descent} не реализовано''')

        return self

iris_dataset = load_iris()
x = iris_dataset['data'][0:100,[0, 2]]
y = iris_dataset['target'][0:100]
y = np.where(y==0, -1, 1)

plt.scatter(x[:50, 0], x[:50, 1], color='red', marker='o', label='Щетинистый')
plt.scatter(x[50:100, 0], x[50:100, 1], color='blue', marker='x', label='Разноцветный')
plt.xlabel('Длина чашелистника (см)')
plt.ylabel('Длина лепестка (см)')
plt.legend()
plt.show()

perceptron = Perceptron(eta=0.01, n_iter=10)
perceptron.fit(x, y)

plt.plot(range(1, len(perceptron.errors_)+1), perceptron.errors_, marker='o')
plt.xlabel('Эпохи')
plt.ylabel('Число случайной ошибочной классификации')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def plotDecisionRegions(x, y, classifier, resolution=0.02):
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    x1_min, x1_max = x[:,0].min() - 1, x[:,0].max() + 1
    x2_min, x2_max = x[:,1].min() - 1, x[:,1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))

    z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    z = z.reshape(xx1.shape)

    plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=x[y==cl, 0], y=x[y==cl, 1], alpha=0.8, c=colors[idx],
                    edgecolor='black', marker = markers[idx], label=cl)

plotDecisionRegions(x, y, perceptron)
plt.xlabel('Sepal length (cm)')
plt.ylabel('Petal length (cm)')
plt.legend()
plt.show()

adaline_batch_1 = AdalineGD(eta=0.0001, n_iter=50, type_grad_descent='Batch')
adaline_batch_1.fit(x, y)

adaline_batch_2 = AdalineGD(eta=0.01, n_iter=10, type_grad_descent='Batch')
adaline_batch_2.fit(x, y)

adaline_batch_3 = AdalineGD(eta=0.0001, n_iter=10, type_grad_descent='Batch')
adaline_batch_3.fit(x, y)

plotDecisionRegions(x, y, adaline_batch_1)
plt.xlabel('Sepal length (cm)')
plt.ylabel('Petal length (cm)')
plt.legend()
plt.show()

plotDecisionRegions(x, y, adaline_batch_2)
plt.xlabel('Sepal length (cm)')
plt.ylabel('Petal length (cm)')
plt.legend()
plt.show()

plotDecisionRegions(x, y, adaline_batch_3)
plt.xlabel('Sepal length (cm)')
plt.ylabel('Petal length (cm)')
plt.legend()
plt.show()

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4))

ax[0].plot(range(1, len(adaline_batch_2.errors_)+1), np.log10(adaline_batch_2.errors_), marker='o')
ax[0].set_xlabel('Эпохи')
ax[0].set_ylabel('log(Сумма квдратичных ошибок)')
ax[0].set_title('Adaline - темп обучения 0.01')

ax[1].plot(range(1, len(adaline_batch_3.errors_)+1), np.log10(adaline_batch_3.errors_), marker='o')
ax[1].set_xlabel('Эпохи')
ax[1].set_ylabel('log(Сумма квдратичных ошибок)')
ax[1].set_title('Adaline - темп обучения 0.0001')
plt.show()

adaline_stochastic = AdalineGD(eta=0.01, n_iter=15, type_grad_descent='Stochastic')
adaline_stochastic.fit(x, y)

plt.plot(range(1, len(adaline_stochastic.errors_)+1), adaline_stochastic.errors_, marker='o')
plt.xlabel('Эпохи')
plt.ylabel('Число случайной ошибочной классификации')
plt.show()
plotDecisionRegions(x, y, adaline_stochastic)
plt.xlabel('Sepal length (cm)')
plt.ylabel('Petal length (cm)')
plt.legend()
plt.show()

